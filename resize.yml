# Resize the cluster. Note this assumes there are no cloud nodes!
# TODO: handle multiple partitions

- name: identify compute nodes to add/remove
  hosts: localhost
  gather_facts: false
  tasks:
    - name: check there are no cloud nodes
      fail:
        msg: "Manual resizing only works on clusters with no cloud nodes"
      when: "ohpc_partitions[0].max_nodes != ohpc_partitions[0].min_nodes"
    - name: identify nodes to delete by comparing inventory against `min_nodes` # NB always last nodes in partition
      set_fact:
        to_delete: "{{ groups['ohpc_compute'][(ohpc_partitions[0].min_nodes - groups['ohpc_compute'] | length):] if ohpc_partitions[0].min_nodes < groups['ohpc_compute'] | length else []}}"
    - name: identify nodes to add by comparing inventory against `min_nodes`
      set_fact:
        to_add: "{{ range(ohpc_partitions[0].min_nodes) | map('regex_replace', '^(.*)$', 'ohpc-compute-\\1') | list | difference(groups['ohpc_compute']) }}"
      # TODO: handle multiple partitions in both the above
    - debug:
        msg: "{{ item }}"
      loop:
        - "{{ 'will delete node(s) ' + ' '.join(to_delete) if to_delete != [] else ''}}" #
        - "{{ 'will add node(s) ' + ' '.join(to_add) if to_add != [] else '' }}"
        - "{{ 'no nodes to be added/deleted' if (to_delete == [] and to_add == []) else ''}}"
      when: "item != ''"

- name: make changes
  gather_facts: false
  hosts:
  - cluster_control
  - cluster_batch
  - cluster_login
  tasks:
    - block:
      - name: run openhpc role to drain nodes  # is actually be safe to run if no nodes need draining but looks odd
        import_role:
          role: stackhpc.openhpc
        vars:
          openhpc_enable:
            drain: "{{ inventory_hostname in hostvars['localhost']['to_delete'] }}"
        become: true
      - name: return drained nodes to cloud and update inventory, slurm.conf and /etc/hosts
        command: "slurmscripts/reconfigure.py delete {{ ' '.join(hostvars['localhost']['to_delete']) }}"
        run_once: true
        delegate_to: localhost
      when: "hostvars['localhost']['to_delete'] != []"
    
    - name: add instances and update inventory, slurm.conf and /etc/hosts
      command: "slurmscripts/reconfigure.py resume {{ ' '.join(hostvars['localhost']['to_add']) }}"
      run_once: true
      delegate_to: localhost
      when: "hostvars['localhost']['to_add'] != []"
    
    - block:
      - meta: refresh_inventory # will have changed on disk
        run_once: true
        
      - name: restart slurm daemons
        service:
          name: "{{ item.value }}"
          state: restarted
        loop: "{{ slurm_service | dict2items }}"
        vars:
          slurm_service:
            cluster_control: slurmctld
            cluster_batch: slurmd
        when: "item.key in group_names"

      # - name: reconfigure slurm
      #   command: "scontrol reconfigure"
      #   run_once: true # expect slurmctld to die with stderr: slurm_reconfigure error: Zero Bytes were transmitted or received

      when: "hostvars['localhost']['to_add'] != [] or hostvars['localhost']['to_delete'] != []"
      become: true
    
    - debug:
        msg: "{{ item }}"
      loop:
        - "{{ 'have delete node(s) ' + ' '.join(hostvars['localhost']['to_delete']) if hostvars['localhost']['to_delete'] != [] else ''}}" #
        - "{{ 'have added node(s) ' + ' '.join(hostvars['localhost']['to_add']) if hostvars['localhost']['to_add'] != [] else '' }}"
        - "{{ 'no nodes added/deleted' if (hostvars['localhost']['to_delete'] == [] and hostvars['localhost']['to_add'] == []) else ''}}"
      when: "item != ''"

